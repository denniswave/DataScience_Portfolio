{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. Extracting Topics\n",
    "For unsupervised learning it is helpfull to be able to extract the topics from the text. Although we are not going to do unsupervised, we might also want to extract the topics to detect new ones and to convert them to the Interpol list. I found a tutorial online that might help and I'll try it in this notebook.\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)\n",
    "\n",
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
       " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at some sample news\n",
    "newsgroups_train.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314,) (11314,)\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.filenames.shape, newsgroups_train.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/16090187/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst'], ['guykuo', 'carson', 'washington', 'subject', 'clock', 'poll', 'final', 'summari', 'final', 'clock', 'report', 'keyword', 'acceler', 'clock', 'upgrad', 'articl', 'shelley', 'qvfo', 'innc', 'organ', 'univers', 'washington', 'line', 'nntp', 'post', 'host', 'carson', 'washington', 'fair', 'number', 'brave', 'soul', 'upgrad', 'clock', 'oscil', 'share', 'experi', 'poll', 'send', 'brief', 'messag', 'detail', 'experi', 'procedur', 'speed', 'attain', 'rat', 'speed', 'card', 'adapt', 'heat', 'sink', 'hour', 'usag', 'floppi', 'disk', 'function', 'floppi', 'especi', 'request', 'summar', 'day', 'network', 'knowledg', 'base', 'clock', 'upgrad', 'haven', 'answer', 'poll', 'thank', 'guykuo', 'washington']]\n"
     ]
    }
   ],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in newsgroups_train.data:\n",
    "    processed_docs.append(preprocess(doc))\n",
    "    \n",
    "print(processed_docs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 addit\n",
      "1 bodi\n",
      "2 bricklin\n",
      "3 bring\n",
      "4 bumper\n",
      "5 call\n",
      "6 colleg\n",
      "7 door\n",
      "8 earli\n",
      "9 engin\n",
      "10 enlighten\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 8, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.007*\"presid\" + 0.005*\"clinton\" + 0.005*\"bike\" + 0.004*\"homosexu\" + 0.004*\"netcom\" + 0.004*\"virginia\" + 0.004*\"run\" + 0.003*\"pitch\" + 0.003*\"talk\" + 0.003*\"consid\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.009*\"govern\" + 0.007*\"armenian\" + 0.006*\"israel\" + 0.005*\"kill\" + 0.005*\"isra\" + 0.004*\"american\" + 0.004*\"turkish\" + 0.004*\"weapon\" + 0.004*\"jew\" + 0.004*\"countri\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.017*\"game\" + 0.015*\"team\" + 0.011*\"play\" + 0.009*\"player\" + 0.008*\"hockey\" + 0.006*\"season\" + 0.005*\"leagu\" + 0.005*\"canada\" + 0.005*\"score\" + 0.004*\"andrew\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.012*\"window\" + 0.011*\"card\" + 0.007*\"driver\" + 0.007*\"drive\" + 0.006*\"sale\" + 0.005*\"control\" + 0.005*\"price\" + 0.005*\"speed\" + 0.005*\"disk\" + 0.005*\"scsi\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.013*\"file\" + 0.009*\"program\" + 0.007*\"window\" + 0.006*\"encrypt\" + 0.006*\"chip\" + 0.006*\"imag\" + 0.006*\"data\" + 0.006*\"avail\" + 0.005*\"code\" + 0.004*\"version\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.012*\"space\" + 0.009*\"nasa\" + 0.006*\"scienc\" + 0.005*\"orbit\" + 0.004*\"research\" + 0.004*\"launch\" + 0.003*\"pitt\" + 0.003*\"food\" + 0.003*\"earth\" + 0.003*\"develop\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.023*\"drive\" + 0.007*\"car\" + 0.006*\"hard\" + 0.006*\"uiuc\" + 0.005*\"columbia\" + 0.004*\"engin\" + 0.004*\"light\" + 0.004*\"colorado\" + 0.004*\"disk\" + 0.004*\"david\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.012*\"christian\" + 0.008*\"jesus\" + 0.006*\"exist\" + 0.005*\"moral\" + 0.005*\"bibl\" + 0.005*\"word\" + 0.005*\"religion\" + 0.004*\"church\" + 0.004*\"life\" + 0.004*\"claim\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: help\n",
      "From: C..Doelle@p26.f3333.n106.z1.fidonet.org (C. Doelle)\n",
      "Lines: 13\n",
      "\n",
      "Hello All!\n",
      "\n",
      "    It is my understanding that all True-Type fonts in Windows are loaded in\n",
      "prior to starting Windows - this makes getting into Windows quite slow if you\n",
      "have hundreds of them as I do.  First off, am I correct in this thinking -\n",
      "secondly, if that is the case - can you get Windows to ignore them on boot and\n",
      "maybe make something like a PIF file to load them only when you enter the\n",
      "applications that need fonts?  Any ideas?\n",
      "\n",
      "\n",
      "Chris\n",
      "\n",
      " * Origin: chris.doelle.@f3333.n106.z1.fidonet.org (1:106/3333.26)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 100\n",
    "unseen_document = newsgroups_test.data[num]\n",
    "print(unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7365520000457764\t Topic: 0.012*\"window\" + 0.011*\"card\" + 0.007*\"driver\" + 0.007*\"drive\" + 0.006*\"sale\"\n",
      "Score: 0.23920853435993195\t Topic: 0.013*\"file\" + 0.009*\"program\" + 0.007*\"window\" + 0.006*\"encrypt\" + 0.006*\"chip\"\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_test.target[num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora\n",
    "After following the tutorial, let's try the topic modelling on our own dataset.\n",
    "\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Structured_DataFrame_Sample_500.csv', index_col=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['emporio', 'armani', 'shell', 'case', 'ceram', 'bracelet', 'replica', 'watch', 'inform', 'brand', 'armani', 'dial', 'window', 'materi', 'type', 'miner', 'band', 'materi', 'ceram', 'case', 'materi', 'ceram', 'case', 'diamet', 'millimet', 'case', 'thick', 'millimet', 'item'], ['cartier', 'tank', 'ladi', 'brand', 'cartier', 'seri', 'tank', 'gender', 'ladi', 'diamet', 'thick', 'movement', 'swiss', 'quartz', 'movement', 'function', 'hour', 'minut', 'second', 'case', 'materi', 'stainless', 'steel', 'strap', 'materi', 'real']]\n"
     ]
    }
   ],
   "source": [
    "processed_descriptions = []\n",
    "\n",
    "for description in df['Item Description']:\n",
    "    processed_descriptions.append(preprocess(description))\n",
    "    \n",
    "print(processed_descriptions[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 armani\n",
      "1 band\n",
      "2 bracelet\n",
      "3 brand\n",
      "4 case\n",
      "5 ceram\n",
      "6 dial\n",
      "7 diamet\n",
      "8 emporio\n",
      "9 inform\n",
      "10 item\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 30, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.105*\"account\" + 0.039*\"lifetim\" + 0.024*\"work\" + 0.021*\"freebi\" + 0.021*\"card\" + 0.020*\"premium\" + 0.019*\"anonym\" + 0.015*\"porn\" + 0.015*\"cash\" + 0.014*\"test\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.034*\"haze\" + 0.027*\"super\" + 0.018*\"amnesia\" + 0.016*\"hydrocodon\" + 0.016*\"sulfat\" + 0.014*\"stealth\" + 0.014*\"escrow\" + 0.013*\"avail\" + 0.013*\"offer\" + 0.013*\"want\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.029*\"ketamin\" + 0.018*\"time\" + 0.014*\"stamp\" + 0.013*\"prioriti\" + 0.013*\"hash\" + 0.013*\"bag\" + 0.012*\"sourc\" + 0.012*\"come\" + 0.011*\"crystal\" + 0.011*\"uncut\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.093*\"cocain\" + 0.061*\"track\" + 0.034*\"shipment\" + 0.033*\"best\" + 0.028*\"offer\" + 0.026*\"number\" + 0.020*\"reship\" + 0.018*\"receiv\" + 0.018*\"stick\" + 0.017*\"sign\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.083*\"heroin\" + 0.073*\"mdma\" + 0.023*\"white\" + 0.020*\"crystal\" + 0.019*\"uncut\" + 0.017*\"black\" + 0.017*\"brown\" + 0.014*\"tramadol\" + 0.014*\"strong\" + 0.012*\"puriti\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.067*\"speed\" + 0.064*\"blotter\" + 0.048*\"past\" + 0.048*\"nbome\" + 0.021*\"amphetamin\" + 0.019*\"best\" + 0.012*\"method\" + 0.011*\"cartridg\" + 0.011*\"freebas\" + 0.011*\"dutch\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.040*\"strain\" + 0.029*\"kush\" + 0.028*\"blue\" + 0.024*\"sativa\" + 0.020*\"indica\" + 0.018*\"indoor\" + 0.017*\"weed\" + 0.015*\"dream\" + 0.015*\"hybrid\" + 0.014*\"grade\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.058*\"tab\" + 0.028*\"testosteron\" + 0.024*\"vial\" + 0.018*\"blotter\" + 0.016*\"tablet\" + 0.014*\"enanth\" + 0.012*\"hash\" + 0.012*\"read\" + 0.012*\"steroid\" + 0.011*\"lay\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.041*\"hash\" + 0.026*\"open\" + 0.021*\"templat\" + 0.019*\"driver\" + 0.019*\"licenc\" + 0.016*\"need\" + 0.014*\"photoshop\" + 0.014*\"click\" + 0.014*\"provid\" + 0.014*\"text\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.071*\"adderal\" + 0.025*\"brand\" + 0.020*\"capsul\" + 0.019*\"chew\" + 0.018*\"refund\" + 0.018*\"releas\" + 0.017*\"amazon\" + 0.017*\"tablet\" + 0.017*\"pill\" + 0.015*\"sale\"\n",
      "\n",
      "\n",
      "Topic: 10 \n",
      "Words: 0.052*\"pill\" + 0.023*\"oxycodon\" + 0.023*\"grade\" + 0.017*\"pharmaceut\" + 0.017*\"price\" + 0.015*\"actavi\" + 0.014*\"best\" + 0.012*\"tablet\" + 0.012*\"cocain\" + 0.010*\"diazepam\"\n",
      "\n",
      "\n",
      "Topic: 11 \n",
      "Words: 0.086*\"mushroom\" + 0.058*\"cubensi\" + 0.041*\"psilocyb\" + 0.039*\"magic\" + 0.026*\"grow\" + 0.023*\"dri\" + 0.022*\"capsul\" + 0.021*\"shroom\" + 0.014*\"potent\" + 0.014*\"strain\"\n",
      "\n",
      "\n",
      "Topic: 12 \n",
      "Words: 0.097*\"nbome\" + 0.033*\"blotter\" + 0.030*\"puriti\" + 0.016*\"effect\" + 0.016*\"psychedel\" + 0.015*\"vendor\" + 0.015*\"white\" + 0.015*\"custom\" + 0.014*\"genuin\" + 0.012*\"phone\"\n",
      "\n",
      "\n",
      "Topic: 13 \n",
      "Words: 0.090*\"case\" + 0.086*\"materi\" + 0.051*\"watch\" + 0.041*\"steel\" + 0.041*\"stainless\" + 0.037*\"brand\" + 0.033*\"millimet\" + 0.032*\"dial\" + 0.031*\"inform\" + 0.031*\"replica\"\n",
      "\n",
      "\n",
      "Topic: 14 \n",
      "Words: 0.027*\"chocol\" + 0.023*\"clean\" + 0.017*\"custom\" + 0.014*\"dutch\" + 0.012*\"kief\" + 0.012*\"mdma\" + 0.011*\"white\" + 0.011*\"seal\" + 0.010*\"easi\" + 0.010*\"hack\"\n",
      "\n",
      "\n",
      "Topic: 15 \n",
      "Words: 0.036*\"vendor\" + 0.025*\"heroin\" + 0.018*\"test\" + 0.018*\"sale\" + 0.017*\"best\" + 0.017*\"escrow\" + 0.016*\"good\" + 0.015*\"offer\" + 0.014*\"price\" + 0.013*\"drive\"\n",
      "\n",
      "\n",
      "Topic: 16 \n",
      "Words: 0.064*\"profil\" + 0.062*\"read\" + 0.026*\"puriti\" + 0.025*\"page\" + 0.021*\"fake\" + 0.019*\"item\" + 0.017*\"powder\" + 0.016*\"term\" + 0.016*\"escrow\" + 0.015*\"main\"\n",
      "\n",
      "\n",
      "Topic: 17 \n",
      "Words: 0.065*\"meth\" + 0.052*\"crystal\" + 0.030*\"price\" + 0.027*\"good\" + 0.021*\"sampl\" + 0.020*\"best\" + 0.013*\"hash\" + 0.013*\"servic\" + 0.013*\"express\" + 0.012*\"methamphetamin\"\n",
      "\n",
      "\n",
      "Topic: 18 \n",
      "Words: 0.062*\"ciali\" + 0.033*\"cooki\" + 0.032*\"send\" + 0.025*\"address\" + 0.022*\"scan\" + 0.019*\"super\" + 0.017*\"option\" + 0.017*\"strong\" + 0.017*\"hash\" + 0.016*\"photo\"\n",
      "\n",
      "\n",
      "Topic: 19 \n",
      "Words: 0.074*\"viagra\" + 0.032*\"pack\" + 0.026*\"organ\" + 0.022*\"avail\" + 0.022*\"cannabi\" + 0.016*\"black\" + 0.014*\"candi\" + 0.014*\"pill\" + 0.013*\"regist\" + 0.013*\"heroin\"\n",
      "\n",
      "\n",
      "Topic: 20 \n",
      "Words: 0.057*\"price\" + 0.027*\"paypal\" + 0.024*\"target\" + 0.023*\"email\" + 0.022*\"direct\" + 0.021*\"proud\" + 0.021*\"link\" + 0.021*\"site\" + 0.021*\"bank\" + 0.020*\"download\"\n",
      "\n",
      "\n",
      "Topic: 21 \n",
      "Words: 0.084*\"custom\" + 0.030*\"blend\" + 0.023*\"card\" + 0.022*\"changa\" + 0.016*\"bitcoin\" + 0.015*\"herbal\" + 0.015*\"oxycontin\" + 0.015*\"special\" + 0.014*\"smoke\" + 0.013*\"dose\"\n",
      "\n",
      "\n",
      "Topic: 22 \n",
      "Words: 0.055*\"shatter\" + 0.018*\"dollar\" + 0.016*\"post\" + 0.014*\"kush\" + 0.014*\"purg\" + 0.013*\"bubba\" + 0.013*\"hour\" + 0.013*\"express\" + 0.013*\"grade\" + 0.012*\"australian\"\n",
      "\n",
      "\n",
      "Topic: 23 \n",
      "Words: 0.056*\"track\" + 0.053*\"worldwid\" + 0.053*\"manufactur\" + 0.052*\"provid\" + 0.050*\"china\" + 0.044*\"iupac\" + 0.034*\"base\" + 0.026*\"signatur\" + 0.019*\"requir\" + 0.018*\"video\"\n",
      "\n",
      "\n",
      "Topic: 24 \n",
      "Words: 0.162*\"fentanyl\" + 0.029*\"acetyl\" + 0.021*\"ritalin\" + 0.020*\"time\" + 0.016*\"potent\" + 0.015*\"blotter\" + 0.015*\"patch\" + 0.014*\"pill\" + 0.014*\"spray\" + 0.014*\"morphin\"\n",
      "\n",
      "\n",
      "Topic: 25 \n",
      "Words: 0.142*\"ketamin\" + 0.019*\"isom\" + 0.019*\"final\" + 0.019*\"earli\" + 0.016*\"escrow\" + 0.014*\"potent\" + 0.013*\"sale\" + 0.012*\"liquid\" + 0.012*\"send\" + 0.012*\"pink\"\n",
      "\n",
      "\n",
      "Topic: 26 \n",
      "Words: 0.078*\"oxycodon\" + 0.041*\"oxycontin\" + 0.031*\"pill\" + 0.028*\"stock\" + 0.017*\"tablet\" + 0.017*\"releas\" + 0.017*\"price\" + 0.014*\"roxi\" + 0.011*\"day\" + 0.011*\"hydrochlorid\"\n",
      "\n",
      "\n",
      "Topic: 27 \n",
      "Words: 0.119*\"xanax\" + 0.055*\"bar\" + 0.049*\"alprazolam\" + 0.023*\"potent\" + 0.018*\"pfizer\" + 0.017*\"powder\" + 0.014*\"liquid\" + 0.014*\"cannabinoid\" + 0.013*\"drug\" + 0.010*\"chemic\"\n",
      "\n",
      "\n",
      "Topic: 28 \n",
      "Words: 0.116*\"seed\" + 0.039*\"femin\" + 0.029*\"cannabi\" + 0.022*\"strain\" + 0.017*\"file\" + 0.016*\"skunk\" + 0.015*\"white\" + 0.014*\"size\" + 0.013*\"auto\" + 0.012*\"widow\"\n",
      "\n",
      "\n",
      "Topic: 29 \n",
      "Words: 0.088*\"http\" + 0.062*\"onion\" + 0.050*\"busi\" + 0.041*\"partner\" + 0.028*\"feedback\" + 0.023*\"price\" + 0.019*\"wmbed\" + 0.018*\"replica\" + 0.016*\"ladi\" + 0.016*\"watch\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I sell some weed for you to smoke and get high!\n"
     ]
    }
   ],
   "source": [
    "num = 100\n",
    "unseen_description = \"Hello, I sell some weed for you to smoke and get high!\"\n",
    "print(unseen_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.26623544096946716\t Topic: 0.041*\"hash\" + 0.026*\"open\" + 0.021*\"templat\" + 0.019*\"driver\" + 0.019*\"licenc\"\n",
      "Score: 0.2602015435695648\t Topic: 0.090*\"case\" + 0.086*\"materi\" + 0.051*\"watch\" + 0.041*\"steel\" + 0.041*\"stainless\"\n",
      "Score: 0.14973457157611847\t Topic: 0.142*\"ketamin\" + 0.019*\"isom\" + 0.019*\"final\" + 0.019*\"earli\" + 0.016*\"escrow\"\n",
      "Score: 0.11855105310678482\t Topic: 0.027*\"chocol\" + 0.023*\"clean\" + 0.017*\"custom\" + 0.014*\"dutch\" + 0.012*\"kief\"\n",
      "Score: 0.10444159060716629\t Topic: 0.057*\"price\" + 0.027*\"paypal\" + 0.024*\"target\" + 0.023*\"email\" + 0.022*\"direct\"\n",
      "Score: 0.07416040450334549\t Topic: 0.062*\"ciali\" + 0.033*\"cooki\" + 0.032*\"send\" + 0.025*\"address\" + 0.022*\"scan\"\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
